{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Your name: Zhiming, SID 3034485754** (Please add your name, and SID to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n",
    "\n",
    "Handout 2/19/2019, due 2/26/2019 by 4pm in Git by committing to your repository.\n",
    "\n",
    "In this homework, we will model covariate shift and attempt to fix it using logistic regression. This is a fairly realistic scenario for data scientists. To keep things well under control and understandable we will use [Fashion-MNIST](http://d2l.ai/chapter_linear-networks/fashion-mnist.html) as the data to experiment on. \n",
    "\n",
    "Follow the instructions from the Fashion MNIST notebook to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import d2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000,)\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for question 1 & 2\n",
    "def logistic_loss(f, y):\n",
    "    l = nd.log(1.0+nd.exp(-f*y))\n",
    "    return l\n",
    "f = nd.arange(-5, 5, 0.01)\n",
    "f.attach_grad()\n",
    "# for y = 1\n",
    "y = nd.ones(shape = f.shape)\n",
    "with autograd.record():\n",
    "    l = logistic_loss(f, y)\n",
    "l.backward()\n",
    "# for loss function\n",
    "plt.figure()\n",
    "plt.title('Logistic loss and its gradient (y=1)')\n",
    "plt.plot(f.asnumpy(), l.asnumpy(), color = 'r',\\\n",
    "         label = 'Loss function')\n",
    "# for grad\n",
    "plt.plot(f.asnumpy(), f.grad.asnumpy(), color = 'b',\\\n",
    "        label = 'Gradient')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = gdata.vision.FashionMNIST(train=True, transform = lambda data, label:\\\n",
    "                                        (data.astype(np.float32), label))\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False, transform = lambda data, label:\\\n",
    "                                        (data.astype(np.float32), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question 3\n",
    "#X, y = train[0:9]\n",
    "# pick out pullover/shirt, and sneaker/scandal\n",
    "# a new preprocess function, can produce biased dataset\n",
    "def preprocess(mnist_train, mnist_test, total_per_label, ratio): # ratio is the lambda above\n",
    "    X, y = mnist_train[:]\n",
    "    # pick up the indices\n",
    "    index_sweater = np.where(y==3)[0]\n",
    "    index_shirt = np.where(y==6)[0]\n",
    "    index_scandal = np.where(y==5)[0]\n",
    "    index_sneaker = np.where(y==7)[0]\n",
    "    # create the class for training, biased\n",
    "    class_sweater = X[index_sweater[0:round(total_per_label*ratio)]]\n",
    "    class_shirt = X[index_shirt[0:round(total_per_label*ratio)]]\n",
    "    class_scandal = X[index_scandal[0:round(total_per_label*(1-ratio))]]\n",
    "    class_sneaker = X[index_sneaker[0:round(total_per_label*(1-ratio))]]\n",
    "    # print(class_sweater.shape, class_shirt.shape, class_scandal.shape, class_sneaker.shape)\n",
    "    train_feature = nd.concat(class_sweater, class_shirt, class_scandal, class_sneaker, dim=0)\n",
    "    \n",
    "    label1 = nd.ones((1, total_per_label)).astype(np.float32)\n",
    "    label2 = nd.zeros((1, total_per_label)).astype(np.float32)\n",
    "    train_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    train_data = gdata.dataset.ArrayDataset(train_feature, train_labels)\n",
    "    # create the class for testing, unbiased\n",
    "    X, y = mnist_test[:]\n",
    "    index1 = np.where(np.logical_or(y==3, y==6))[0]\n",
    "    index2 = np.where(np.logical_or(y==5, y==7))[0]\n",
    "    class1 = X[index1]\n",
    "    class2 = X[index2]\n",
    "    test_feature = nd.concat(class1, class2, dim=0)\n",
    "    \n",
    "    label1 = nd.ones((1, 2000)).astype(np.float32)\n",
    "    label2 = nd.zeros((1, 2000)).astype(np.float32)\n",
    "    test_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    test_data = gdata.dataset.ArrayDataset(test_feature, test_labels)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mnist(train_data, test_data, batch_size, lr, num_epochs):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(2))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    #loss = logistic_loss\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    d2l.train_ch3(net, train_data, test_data, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 16637.0955, train acc 0.420, test acc 0.911\n",
      "epoch 2, loss 1097.2720, train acc 0.960, test acc 0.950\n",
      "epoch 3, loss 239.5708, train acc 0.990, test acc 0.953\n",
      "epoch 4, loss 140.1334, train acc 0.990, test acc 0.958\n",
      "epoch 5, loss 40.6960, train acc 0.990, test acc 0.963\n",
      "epoch 1, loss 461.2452, train acc 0.993, test acc 0.998\n",
      "epoch 2, loss 24.4931, train acc 0.999, test acc 0.999\n",
      "epoch 3, loss 13.6120, train acc 0.999, test acc 0.999\n",
      "epoch 4, loss 8.5127, train acc 0.999, test acc 0.999\n",
      "epoch 5, loss 10.3028, train acc 0.999, test acc 0.999\n"
     ]
    }
   ],
   "source": [
    "# half the dataset \n",
    "# note: use half the dataset, test acc is almost the same as using the full dataset\n",
    "# so I used just 50 per label for train (total 100 for training), which leads to\n",
    "# observable difference in test acc (~.975 v.s. ~.999)\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=50, ratio=0.5) # ratio=.5 for equally division\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.1, num_epochs=5)\n",
    "# full dataset\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=12000, ratio=0.5) # ration=.5 for equally division\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.1, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $600$ `sweater` images and likewise $5,400$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias ratio now is: 0.05\n",
      "epoch 1, loss 988.9615, train acc 0.825, test acc 0.709\n",
      "epoch 2, loss 452.8739, train acc 0.878, test acc 0.790\n",
      "epoch 3, loss 455.8028, train acc 0.882, test acc 0.743\n",
      "epoch 4, loss 485.5297, train acc 0.885, test acc 0.783\n",
      "epoch 5, loss 451.8702, train acc 0.879, test acc 0.756\n",
      "Bias ratio now is: 0.1 \n",
      "epoch 1, loss 1028.6509, train acc 0.802, test acc 0.770\n",
      "epoch 2, loss 700.9965, train acc 0.842, test acc 0.808\n",
      "epoch 3, loss 673.2709, train acc 0.846, test acc 0.826\n",
      "epoch 4, loss 649.5951, train acc 0.846, test acc 0.786\n",
      "epoch 5, loss 632.5607, train acc 0.846, test acc 0.790\n",
      "Bias ratio now is: 0.2 \n",
      "epoch 1, loss 1074.8852, train acc 0.776, test acc 0.943\n",
      "epoch 2, loss 827.0725, train acc 0.798, test acc 0.763\n",
      "epoch 3, loss 825.1936, train acc 0.800, test acc 0.747\n",
      "epoch 4, loss 738.3895, train acc 0.811, test acc 0.809\n",
      "epoch 5, loss 740.9231, train acc 0.808, test acc 0.975\n",
      "Bias ratio now is: 0.3 \n",
      "epoch 1, loss 917.2764, train acc 0.785, test acc 0.858\n",
      "epoch 2, loss 795.5549, train acc 0.802, test acc 0.908\n",
      "epoch 3, loss 675.8966, train acc 0.810, test acc 0.784\n",
      "epoch 4, loss 697.7188, train acc 0.815, test acc 0.968\n",
      "epoch 5, loss 673.9244, train acc 0.809, test acc 0.902\n",
      "Bias ratio now is: 0.4 \n",
      "epoch 1, loss 516.9013, train acc 0.852, test acc 0.991\n",
      "epoch 2, loss 457.1445, train acc 0.860, test acc 0.849\n",
      "epoch 3, loss 415.3252, train acc 0.864, test acc 0.971\n",
      "epoch 4, loss 417.8364, train acc 0.864, test acc 0.840\n",
      "epoch 5, loss 468.7624, train acc 0.859, test acc 0.991\n",
      "Bias ratio now is: 0.5 (unbiased)\n",
      "epoch 1, loss 75.9369, train acc 0.989, test acc 0.997\n",
      "epoch 2, loss 2.6166, train acc 0.998, test acc 0.997\n",
      "epoch 3, loss 1.8166, train acc 0.999, test acc 0.999\n",
      "epoch 4, loss 0.8882, train acc 0.999, test acc 0.999\n",
      "epoch 5, loss 0.5451, train acc 0.999, test acc 0.999\n",
      "Bias ratio now is: 0.6 \n",
      "epoch 1, loss 2079.3587, train acc 0.835, test acc 0.995\n",
      "epoch 2, loss 1714.1571, train acc 0.851, test acc 0.926\n",
      "epoch 3, loss 1741.7172, train acc 0.854, test acc 0.957\n",
      "epoch 4, loss 1724.1075, train acc 0.855, test acc 0.994\n",
      "epoch 5, loss 1728.7022, train acc 0.854, test acc 0.978\n",
      "Bias ratio now is: 0.7 \n",
      "epoch 1, loss 3280.3295, train acc 0.779, test acc 0.858\n",
      "epoch 2, loss 2836.4077, train acc 0.797, test acc 0.889\n",
      "epoch 3, loss 3114.6034, train acc 0.796, test acc 0.776\n",
      "epoch 4, loss 2904.3172, train acc 0.804, test acc 0.814\n",
      "epoch 5, loss 2949.0929, train acc 0.799, test acc 0.892\n",
      "Bias ratio now is: 0.8 \n",
      "epoch 1, loss 3278.5111, train acc 0.771, test acc 0.785\n",
      "epoch 2, loss 2856.5634, train acc 0.795, test acc 0.791\n",
      "epoch 3, loss 2825.1985, train acc 0.797, test acc 0.767\n",
      "epoch 4, loss 2304.8517, train acc 0.809, test acc 0.751\n",
      "epoch 5, loss 2601.6752, train acc 0.799, test acc 0.803\n",
      "Bias ratio now is: 0.95\n",
      "epoch 1, loss 1987.0878, train acc 0.849, test acc 0.733\n",
      "epoch 2, loss 1462.2143, train acc 0.867, test acc 0.765\n",
      "epoch 3, loss 1220.7535, train acc 0.878, test acc 0.757\n",
      "epoch 4, loss 1217.8690, train acc 0.878, test acc 0.745\n",
      "epoch 5, loss 1115.0345, train acc 0.880, test acc 0.745\n"
     ]
    }
   ],
   "source": [
    "total_per_label = 6000\n",
    "num_epochs = 5\n",
    "print(\"Bias ratio now is:\", .05)\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=total_per_label, ratio=0.05)\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.01, num_epochs=num_epochs)\n",
    "for r in range(1, 9, 1):\n",
    "    ratio = r / 10.0\n",
    "    print(\"Bias ratio now is:\", ratio, \"(unbiased)\" if ratio==.5 else \"\")\n",
    "    train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=total_per_label, ratio=ratio)\n",
    "    batch_size = 64\n",
    "    train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.01, num_epochs=num_epochs)\n",
    "print(\"Bias ratio now is:\", .95)\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=total_per_label, ratio=0.95)\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.01, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function with ratio\n",
    "def logistic_loss(f, y):\n",
    "    f = f.reshape(shape=y.shape)\n",
    "    # weight = nd.zeros(shape=y.shape)\n",
    "    # weight[np.where(y==1)[0]] = .5/ratio\n",
    "    # weight[np.where(y==-1)[0]] = .5/(1-ratio)\n",
    "    #l = weight * nd.log(1.0+nd.exp(-y*f))\n",
    "    l = nd.log(nd.ones(shape=y.shape)+nd.exp(-f*y))\n",
    "    return l\n",
    "# calculate acc per epoch\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (nd.sign(net(X)) == y).sum().asscalar()\n",
    "        n += y.size\n",
    "    return acc_sum / n\n",
    "# train weighted version\n",
    "def train_weighted(net, train_iter, test_iter, num_epochs, batch_size, trainer, ratio):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = logistic_loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "# convenient api for train and test\n",
    "def train_and_test_mnist(train_data, test_data, batch_size, lr, num_epochs, ratio):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    #loss = logistic_loss\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    train_weighted(net, train_data, test_data, num_epochs, batch_size, trainer, ratio)\n",
    "# a new preprocess procedure, -1/1 for two labels\n",
    "def preprocess(mnist_train, mnist_test, total_per_label, ratio): # ration is the lambda above\n",
    "    X, y = mnist_train[:]\n",
    "    # pick up the indices\n",
    "    index_sweater = np.where(y==3)[0]\n",
    "    index_shirt = np.where(y==6)[0]\n",
    "    index_scandal = np.where(y==5)[0]\n",
    "    index_sneaker = np.where(y==7)[0]\n",
    "    # create the class for training, biased\n",
    "    class_sweater = X[index_sweater[0:round(total_per_label*ratio)]]\n",
    "    class_shirt = X[index_shirt[0:round(total_per_label*ratio)]]\n",
    "    class_scandal = X[index_scandal[0:round(total_per_label*(1-ratio))]]\n",
    "    class_sneaker = X[index_sneaker[0:round(total_per_label*(1-ratio))]]\n",
    "    # print(class_sweater.shape, class_shirt.shape, class_scandal.shape, class_sneaker.shape)\n",
    "    train_feature = nd.concat(class_sweater, class_shirt, class_scandal, class_sneaker, dim=0)\n",
    "    \n",
    "    label1 = nd.ones((1, total_per_label)).astype(np.float32)\n",
    "    label2 = -nd.ones((1, total_per_label)).astype(np.float32)\n",
    "    train_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    train_data = gdata.dataset.ArrayDataset(train_feature, train_labels)\n",
    "    # create the class for testing, unbiased\n",
    "    X, y = mnist_test[:]\n",
    "    index1 = np.where(np.logical_or(y==3, y==6))[0]\n",
    "    index2 = np.where(np.logical_or(y==5, y==7))[0]\n",
    "    class1 = X[index1]\n",
    "    class2 = X[index2]\n",
    "    test_feature = nd.concat(class1, class2, dim=0)\n",
    "    \n",
    "    label1 = nd.ones((1, 2000)).astype(np.float32)\n",
    "    label2 = -nd.ones((1, 2000)).astype(np.float32)\n",
    "    test_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    test_data = gdata.dataset.ArrayDataset(test_feature, test_labels)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss nan, train acc 0.000, test acc 0.000\n",
      "epoch 2, loss nan, train acc 0.000, test acc 0.000\n",
      "epoch 3, loss nan, train acc 0.000, test acc 0.000\n",
      "epoch 4, loss nan, train acc 0.000, test acc 0.000\n",
      "epoch 5, loss nan, train acc 0.000, test acc 0.000\n"
     ]
    }
   ],
   "source": [
    "total_per_label = 300\n",
    "ratio = 0.9\n",
    "num_epochs = 5\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label, ratio)\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.001, num_epochs=num_epochs, ratio=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
