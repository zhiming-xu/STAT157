{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Your name: Zhiming, SID 3034485754** (Please add your name, and SID to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n",
    "\n",
    "Handout 2/19/2019, due 2/26/2019 by 4pm in Git by committing to your repository.\n",
    "\n",
    "In this homework, we will model covariate shift and attempt to fix it using logistic regression. This is a fairly realistic scenario for data scientists. To keep things well under control and understandable we will use [Fashion-MNIST](http://d2l.ai/chapter_linear-networks/fashion-mnist.html) as the data to experiment on. \n",
    "\n",
    "Follow the instructions from the Fashion MNIST notebook to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import d2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for question 1 & 2\n",
    "def logistic_loss(f, y):\n",
    "    l = nd.log(1.0+nd.exp(-f*y))\n",
    "    return l\n",
    "f = nd.arange(-5, 5, 0.01)\n",
    "f.attach_grad()\n",
    "# for y = 1\n",
    "y = nd.ones(shape = f.shape)\n",
    "with autograd.record():\n",
    "    l = logistic_loss(f, y)\n",
    "l.backward()\n",
    "# for loss function\n",
    "plt.figure()\n",
    "plt.title('Logistic loss and its gradient (y=1)')\n",
    "plt.plot(f.asnumpy(), l.asnumpy(), color = 'r',\\\n",
    "         label = 'Loss function')\n",
    "# for grad\n",
    "plt.plot(f.asnumpy(), f.grad.asnumpy(), color = 'b',\\\n",
    "        label = 'Gradient')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = gdata.vision.FashionMNIST(train=True, transform = lambda data, label:\\\n",
    "                                        (data.astype(np.float32), label))\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False, transform = lambda data, label:\\\n",
    "                                        (data.astype(np.float32), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question 3\n",
    "#X, y = train[0:9]\n",
    "# pick out pullover/shirt, and sneaker/scandal\n",
    "# a new preprocess function, can produce biased dataset\n",
    "def preprocess(mnist_train, mnist_test, total_per_label, ratio): # ratio is the lambda above\n",
    "    X, y = mnist_train[:]\n",
    "    # pick up the indices\n",
    "    index_sweater = np.where(y==3)[0]\n",
    "    index_shirt = np.where(y==6)[0]\n",
    "    index_scandal = np.where(y==5)[0]\n",
    "    index_sneaker = np.where(y==7)[0]\n",
    "    # create the class for training, biased\n",
    "    class_sweater = X[index_sweater[0:round(total_per_label*ratio)]]\n",
    "    class_sneaker = X[index_sneaker[0:round(total_per_label*ratio)]]\n",
    "    class_scandal = X[index_scandal[0:round(total_per_label*(1-ratio))]]\n",
    "    class_shirt = X[index_shirt[0:round(total_per_label*(1-ratio))]]\n",
    "    # print(class_sweater.shape, class_shirt.shape, class_scandal.shape, class_sneaker.shape)\n",
    "    train_feature = nd.concat(class_sweater, class_shirt, class_scandal, class_sneaker, dim=0)\n",
    "    label1 = nd.ones((1, round(total_per_label*2*ratio))).astype(np.float32)\n",
    "    label2 = nd.zeros((1, round(total_per_label*2*(1-ratio)))).astype(np.float32)\n",
    "    train_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    train_data = gdata.dataset.ArrayDataset(train_feature, train_labels)\n",
    "    # create the class for testing, unbiased\n",
    "    X, y = mnist_test[:]\n",
    "    index1 = np.where(np.logical_or(y==3, y==6))[0]\n",
    "    index2 = np.where(np.logical_or(y==5, y==7))[0]\n",
    "    class1 = X[index1]\n",
    "    class2 = X[index2]\n",
    "    test_feature = nd.concat(class1, class2, dim=0)\n",
    "    \n",
    "    label1 = nd.ones((1, 2000)).astype(np.float32)\n",
    "    label2 = nd.zeros((1, 2000)).astype(np.float32)\n",
    "    test_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    test_data = gdata.dataset.ArrayDataset(test_feature, test_labels)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_mnist(train_data, test_data, batch_size, lr, num_epochs):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(2))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    #loss = logistic_loss\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    d2l.train_ch3(net, train_data, test_data, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 26718.0885, train acc 0.660, test acc 0.526\n",
      "epoch 2, loss 11235.9929, train acc 0.670, test acc 0.975\n",
      "epoch 3, loss 11.1763, train acc 0.990, test acc 0.980\n",
      "epoch 4, loss 0.0000, train acc 1.000, test acc 0.980\n",
      "epoch 5, loss 0.0000, train acc 1.000, test acc 0.980\n",
      "epoch 1, loss 435.3934, train acc 0.992, test acc 0.997\n",
      "epoch 2, loss 44.4580, train acc 0.998, test acc 0.999\n",
      "epoch 3, loss 26.6787, train acc 0.999, test acc 0.999\n",
      "epoch 4, loss 17.2049, train acc 0.999, test acc 0.999\n",
      "epoch 5, loss 15.4026, train acc 0.999, test acc 0.999\n"
     ]
    }
   ],
   "source": [
    "# half the dataset \n",
    "# note: use half the dataset, test acc is almost the same as using the full dataset\n",
    "# so I used just 50 per label for train (total 100 for training), which leads to\n",
    "# observable difference in test acc (~.975 v.s. ~.999)\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=50, ratio=0.5) # ratio=.5 for equally division\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.1, num_epochs=5)\n",
    "# full dataset\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=12000, ratio=0.5) # ration=.5 for equally division\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.1, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $600$ `sweater` images and likewise $5,400$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias ratio now is: 0.05\n",
      "epoch 1, loss 1000.1986, train acc 0.924, test acc 0.724\n",
      "epoch 2, loss 724.9604, train acc 0.934, test acc 0.714\n",
      "epoch 3, loss 743.7490, train acc 0.935, test acc 0.695\n",
      "epoch 4, loss 698.5812, train acc 0.934, test acc 0.505\n",
      "epoch 5, loss 630.6260, train acc 0.940, test acc 0.697\n",
      "epoch 6, loss 655.4589, train acc 0.938, test acc 0.718\n",
      "epoch 7, loss 689.7992, train acc 0.935, test acc 0.742\n",
      "epoch 8, loss 739.3488, train acc 0.934, test acc 0.672\n",
      "Bias ratio now is: 0.1 \n",
      "epoch 1, loss 1718.0855, train acc 0.872, test acc 0.655\n",
      "epoch 2, loss 1478.2894, train acc 0.882, test acc 0.599\n",
      "epoch 3, loss 1485.8477, train acc 0.887, test acc 0.799\n",
      "epoch 4, loss 1429.8997, train acc 0.888, test acc 0.642\n",
      "epoch 5, loss 1307.3289, train acc 0.888, test acc 0.538\n",
      "epoch 6, loss 1196.0399, train acc 0.890, test acc 0.933\n",
      "epoch 7, loss 1389.5948, train acc 0.889, test acc 0.757\n",
      "epoch 8, loss 1321.6712, train acc 0.890, test acc 0.578\n",
      "Bias ratio now is: 0.2 \n",
      "epoch 1, loss 2631.4459, train acc 0.804, test acc 0.715\n",
      "epoch 2, loss 2487.7274, train acc 0.821, test acc 0.772\n",
      "epoch 3, loss 2439.9909, train acc 0.815, test acc 0.706\n",
      "epoch 4, loss 2325.9607, train acc 0.824, test acc 0.870\n",
      "epoch 5, loss 2220.9590, train acc 0.824, test acc 0.723\n",
      "epoch 6, loss 2325.0559, train acc 0.824, test acc 0.741\n",
      "epoch 7, loss 2409.2014, train acc 0.821, test acc 0.724\n",
      "epoch 8, loss 2370.7292, train acc 0.825, test acc 0.695\n",
      "Bias ratio now is: 0.3 \n",
      "epoch 1, loss 3033.8086, train acc 0.786, test acc 0.804\n",
      "epoch 2, loss 2806.9189, train acc 0.800, test acc 0.823\n",
      "epoch 3, loss 2711.9641, train acc 0.808, test acc 0.962\n",
      "epoch 4, loss 2621.7852, train acc 0.809, test acc 0.994\n",
      "epoch 5, loss 2775.3984, train acc 0.806, test acc 0.707\n",
      "epoch 6, loss 2596.9733, train acc 0.812, test acc 0.758\n",
      "epoch 7, loss 2704.1785, train acc 0.809, test acc 0.836\n",
      "epoch 8, loss 2663.2411, train acc 0.809, test acc 0.962\n",
      "Bias ratio now is: 0.4 \n",
      "epoch 1, loss 1885.2455, train acc 0.840, test acc 0.971\n",
      "epoch 2, loss 1816.4795, train acc 0.853, test acc 0.965\n",
      "epoch 3, loss 1656.1928, train acc 0.855, test acc 0.910\n",
      "epoch 4, loss 1605.6959, train acc 0.859, test acc 0.788\n",
      "epoch 5, loss 1773.3373, train acc 0.853, test acc 0.716\n",
      "epoch 6, loss 1714.5210, train acc 0.857, test acc 0.915\n",
      "epoch 7, loss 1636.6776, train acc 0.862, test acc 0.847\n",
      "epoch 8, loss 1834.5523, train acc 0.851, test acc 0.991\n",
      "Bias ratio now is: 0.5 (unbiased)\n",
      "epoch 1, loss 46.6982, train acc 0.991, test acc 0.998\n",
      "epoch 2, loss 1.8334, train acc 0.999, test acc 0.999\n",
      "epoch 3, loss 1.1898, train acc 0.999, test acc 0.999\n",
      "epoch 4, loss 0.4084, train acc 1.000, test acc 0.999\n",
      "epoch 5, loss 0.2590, train acc 1.000, test acc 0.999\n",
      "epoch 6, loss 0.1962, train acc 0.999, test acc 0.999\n",
      "epoch 7, loss 0.1693, train acc 1.000, test acc 0.999\n",
      "epoch 8, loss 0.0618, train acc 1.000, test acc 0.999\n",
      "Bias ratio now is: 0.6 \n",
      "epoch 1, loss 354.8648, train acc 0.878, test acc 0.982\n",
      "epoch 2, loss 270.1231, train acc 0.889, test acc 0.861\n",
      "epoch 3, loss 237.8066, train acc 0.895, test acc 0.897\n",
      "epoch 4, loss 267.0840, train acc 0.891, test acc 0.938\n",
      "epoch 5, loss 252.3673, train acc 0.895, test acc 0.943\n",
      "epoch 6, loss 262.2170, train acc 0.893, test acc 0.900\n",
      "epoch 7, loss 228.3260, train acc 0.897, test acc 0.840\n",
      "epoch 8, loss 297.1961, train acc 0.894, test acc 0.864\n",
      "Bias ratio now is: 0.7 \n",
      "epoch 1, loss 787.2096, train acc 0.852, test acc 0.766\n",
      "epoch 2, loss 596.7233, train acc 0.871, test acc 0.785\n",
      "epoch 3, loss 596.5825, train acc 0.878, test acc 0.771\n",
      "epoch 4, loss 528.9966, train acc 0.883, test acc 0.773\n",
      "epoch 5, loss 545.9571, train acc 0.883, test acc 0.647\n",
      "epoch 6, loss 488.5068, train acc 0.887, test acc 0.756\n",
      "epoch 7, loss 553.4973, train acc 0.881, test acc 0.760\n",
      "epoch 8, loss 502.5286, train acc 0.887, test acc 0.860\n",
      "Bias ratio now is: 0.8 \n",
      "epoch 1, loss 1869.1117, train acc 0.769, test acc 0.765\n",
      "epoch 2, loss 1853.5029, train acc 0.778, test acc 0.503\n",
      "epoch 3, loss 1641.7631, train acc 0.790, test acc 0.603\n",
      "epoch 4, loss 1715.6684, train acc 0.786, test acc 0.554\n",
      "epoch 5, loss 1706.5300, train acc 0.787, test acc 0.652\n",
      "epoch 6, loss 1708.6808, train acc 0.789, test acc 0.753\n",
      "epoch 7, loss 1668.6962, train acc 0.789, test acc 0.764\n",
      "epoch 8, loss 1601.7086, train acc 0.795, test acc 0.524\n",
      "Bias ratio now is: 0.95\n",
      "epoch 1, loss 834.2551, train acc 0.907, test acc 0.500\n",
      "epoch 2, loss 721.4573, train acc 0.913, test acc 0.507\n",
      "epoch 3, loss 809.8640, train acc 0.910, test acc 0.504\n",
      "epoch 4, loss 695.2549, train acc 0.911, test acc 0.507\n",
      "epoch 5, loss 648.2086, train acc 0.913, test acc 0.500\n",
      "epoch 6, loss 723.6950, train acc 0.911, test acc 0.514\n",
      "epoch 7, loss 766.9381, train acc 0.910, test acc 0.500\n",
      "epoch 8, loss 749.4564, train acc 0.910, test acc 0.500\n"
     ]
    }
   ],
   "source": [
    "total_per_label = 6000\n",
    "num_epochs = 8\n",
    "print(\"Bias ratio now is:\", .05)\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=total_per_label, ratio=0.05)\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.01, num_epochs=num_epochs)\n",
    "for r in range(1, 9, 1):\n",
    "    ratio = r / 10.0\n",
    "    print(\"Bias ratio now is:\", ratio, \"(unbiased)\" if ratio==.5 else \"\")\n",
    "    train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=total_per_label, ratio=ratio)\n",
    "    batch_size = 64\n",
    "    train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.01, num_epochs=num_epochs)\n",
    "print(\"Bias ratio now is:\", .95)\n",
    "train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label=total_per_label, ratio=0.95)\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_and_test_mnist(train_data, test_data, batch_size=batch_size, lr=0.01, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "1. We need to minimize the loss function when the data come from a unbiased dataset, where the label x has a distribution function $p(x)$: \n",
    "$$\\min_w\\int{dxp(x)}\\int{dyp(y|x)l(f(x,w),y)}$$\n",
    "where $p(x)$ is the 'correct' distribution of label $x$ and $q(x)$ is biased.   \n",
    "While the empirical form could be descirbed as:\n",
    "$$\\min_x\\frac{1}{n}\\sum_{i}(l(x_i,y_i),f(x_i))$$\n",
    "When the data come from biased dataset (here is the training set, where $x$ has the distribution $q(x)$, the formula becomes:\n",
    "$$\\min_w\\int{dxq(x)}\\int{dyp(y|x)l(f(x,w),y)}$$\n",
    "So re-weighting data to adjust the $q(x)$ back to $p(x)$ would help the function become unbiased. Therefore, the weight when training the binary classification should be:\n",
    "$$\\beta(x) = \\frac{p(x)}{q(x)} $$\n",
    "The joint probability distribution is (label data from train as -1, from test as 1:\n",
    "$$r(x,y) = \\frac{N_{test}}{N}p(x)\\delta(y,1) + \\frac{N_{train}}{N}q(x)\\delta(y,-1)$$\n",
    "$$r(y = 1|x) = \\frac{r(x,y=1)}{r(x)} = \\frac{r(x,y=1)}{r(x|y=1) + r(x|y=-1)} = \\frac{r(x,y=1)}{r(x, y=1) + r(x, y=-1)} = \\frac{N_{test}p(x)}{N_{test}p(x)+N_{train}q(x)}$$\n",
    "$$r(y = -1|x) = \\frac{r(x,y=-1)}{r(x)} = \\frac{r(x,y=-1)}{r(x|y=-1) + r(x|y=1)} = \\frac{r(x,y=-1)}{r(x, y=1) + r(x, y=-1)} = \\frac{N_{train}q(x)}{N_{test}p(x)+N_{train}q(x)}$$\n",
    "$$\\beta(x) = \\frac{p(x)}{q(x)} = \\frac{r(y = 1|x)}{r(y = -1|x)}*\\frac{N_{train}}{N_{test}} = \\frac{N_{train}}{N_{test}} exp(f(x))$$\n",
    "Therefore, the weight is: $$\\frac{N_{train}}{N_{test}}exp(f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For question 2, weighted loss for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preperations\n",
    "def logistic(z):\n",
    "    return 1. / (1. + nd.exp(-z))\n",
    "# loss function, with weight\n",
    "def log_loss(output, y, ratio):\n",
    "    yhat = logistic(output)\n",
    "    yhat = yhat.reshape(shape=y.shape)\n",
    "    # biased loss\n",
    "    return  - nd.nansum((1-ratio)/(.5+ratio) * y * nd.log(yhat) + ratio/(.5+ratio) * (1-y) * nd.log(1-yhat))\n",
    "    # original loss\n",
    "    # return  - nd.nansum(y * nd.log(yhat) + (1-y) * nd.log(1-yhat))\n",
    "# train_model\n",
    "def train_model(epochs, train_data, net, trainer, batch_size, ratio, f=None):\n",
    "    for e in range(epochs):\n",
    "        cumulative_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            with autograd.record():\n",
    "                if f is not None:\n",
    "                    ratio = f(data)\n",
    "                    print('now the ratio is:', ratio)\n",
    "                output = net(data)\n",
    "                # print('weigth of net:', net.bias.data(), net.weight.data())\n",
    "                # print('output of net:', output)\n",
    "                loss = log_loss(output, label, ratio)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "        print(\"Epoch %s, loss: %s\" % (e + 1, cumulative_loss ))\n",
    "# test_model\n",
    "def test_model(test_data):\n",
    "    num_correct = 0.0\n",
    "    num_total = 0\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        num_total += len(label)\n",
    "        output = net(data)\n",
    "        # visual = nd.concat(output.reshape(shape=(64,1)), label.reshape(shape=(64,1)), dim=1)\n",
    "        # print(visual)\n",
    "        prediction = ((nd.sign(output).reshape(shape=label.shape) + 1) / 2)\n",
    "        num_correct += nd.sum(prediction == label)\n",
    "    print(\"Accuracy: %0.3f (%s/%s)\" % (num_correct.asscalar()/num_total, num_correct.asscalar(), num_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine train and test\n",
    "def train_and_test_mnist_ratio(train_data, test_data, net, trainer, num_epochs, batch_size, ratio):\n",
    "    train_data, test_data = preprocess(mnist_train, mnist_test, total_per_label, ratio)\n",
    "    train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    train_model(num_epochs, train_data, net, trainer, batch_size, ratio)\n",
    "    test_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mnist_train, mnist_test, total_per_label, ratio): # ration is the lambda above\n",
    "    X, y = mnist_train[:]\n",
    "    # pick up the indices\n",
    "    index_sweater = np.where(y==3)[0]\n",
    "    index_shirt = np.where(y==6)[0]\n",
    "    index_scandal = np.where(y==5)[0]\n",
    "    index_sneaker = np.where(y==7)[0]\n",
    "    # create the class for training, biased\n",
    "    class_sweater = X[index_sweater[0:round(total_per_label*ratio)]]\n",
    "    class_shirt = X[index_shirt[0:round(total_per_label*ratio)]]\n",
    "    class_scandal = X[index_scandal[0:round(total_per_label*(1-ratio))]]\n",
    "    class_sneaker = X[index_sneaker[0:round(total_per_label*(1-ratio))]]\n",
    "    # print(class_sweater.shape, class_shirt.shape, class_scandal.shape, class_sneaker.shape)\n",
    "    train_feature = nd.concat(class_sweater, class_shirt, class_scandal, class_sneaker, dim=0)\n",
    "    train_feature = nd.flatten(train_feature)\n",
    "    label = nd.ones((1, total_per_label*2)).astype(np.float32)\n",
    "    train_labels = label.reshape(shape=(-1,))\n",
    "    train_data = gdata.dataset.ArrayDataset(train_feature, train_labels)\n",
    "    # create the class for testing, unbiased\n",
    "    X, y = mnist_test[:]\n",
    "    index1 = np.where(np.logical_or(y==3, y==6))[0]\n",
    "    index2 = np.where(np.logical_or(y==5, y==7))[0]\n",
    "    class1 = X[index1]\n",
    "    class2 = X[index2]\n",
    "    test_feature = nd.concat(class1, class2, dim=0)\n",
    "    test_feature = nd.flatten(test_feature)\n",
    "    label = nd.zeros((1, 4000)).astype(np.float32)\n",
    "    test_labels = label.reshape(shape=(-1,))\n",
    "    test_data = gdata.dataset.ArrayDataset(test_feature, test_labels)\n",
    "    return train_data, test_data\n",
    "def generator(ratio, batch_size):\n",
    "    mnist_train = gdata.vision.FashionMNIST(train=True,transform=lambda data, label: (data.astype(np.float32)/255.0, label))\n",
    "    mnist_test = gdata.vision.FashionMNIST(train=False,transform=lambda data, label: (data.astype(np.float32)/255.0, label))\n",
    "    bias_train, bias_test = preprocess(mnist_train, mnist_test, 6000, ratio)\n",
    "    #train's label should be 1 and test's label should be 0\n",
    "    f_train, l_train = bias_train[:]\n",
    "    f_test, l_test = bias_test[:]\n",
    "    \n",
    "    l_train = nd.ones((1, 12000)).astype(np.float32)\n",
    "    l_test = nd.zeros((1,4000)).astype(np.float32)\n",
    "    \n",
    "    trainLabel = nd.concat(l_train[:,:8000],l_test[:,:3000],dim = 1).reshape(shape=(-1,))\n",
    "    testLabel = nd.concat(l_train[:,8000:],l_test[:,3000:],dim = 1).reshape(shape=(-1,))\n",
    "    trainFeature = nd.concat(f_train[:8000], f_test[:3000], dim = 0)\n",
    "    testFeature = nd.concat(f_train[8000:], f_test[3000:], dim = 0)\n",
    "\n",
    "    train_data = gdata.dataset.ArrayDataset(trainFeature, trainLabel) \n",
    "    test_data = gdata.dataset.ArrayDataset(testFeature, testLabel) \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 3574.4570150375366\n",
      "Epoch 2, loss: 3151.593458175659\n",
      "Epoch 3, loss: 3070.2279958724976\n",
      "Epoch 4, loss: 3023.494716644287\n",
      "Epoch 5, loss: 2990.501064300537\n",
      "Epoch 6, loss: 2964.288585662842\n",
      "Epoch 7, loss: 2943.6569423675537\n",
      "Epoch 8, loss: 2925.1764526367188\n",
      "Epoch 9, loss: 2907.9407787323\n",
      "Epoch 10, loss: 2893.4413805007935\n",
      "Epoch 11, loss: 2879.601933479309\n",
      "Epoch 12, loss: 2867.217098236084\n",
      "Epoch 13, loss: 2856.6793184280396\n",
      "Epoch 14, loss: 2846.189125061035\n",
      "Epoch 15, loss: 2836.723868370056\n",
      "Epoch 16, loss: 2826.2606382369995\n",
      "Epoch 17, loss: 2817.787402153015\n",
      "Epoch 18, loss: 2810.8221797943115\n",
      "Epoch 19, loss: 2804.0963249206543\n",
      "Epoch 20, loss: 2796.3563499450684\n",
      "Epoch 21, loss: 2788.6778297424316\n",
      "Epoch 22, loss: 2783.7293033599854\n",
      "Epoch 23, loss: 2777.1588201522827\n",
      "Epoch 24, loss: 2771.2103633880615\n",
      "Epoch 25, loss: 2766.2930002212524\n",
      "Epoch 26, loss: 2760.329577445984\n",
      "Epoch 27, loss: 2756.365584373474\n",
      "Epoch 28, loss: 2751.379147529602\n",
      "Epoch 29, loss: 2747.0357389450073\n",
      "Epoch 30, loss: 2742.0883922576904\n",
      "Epoch 31, loss: 2738.3803358078003\n",
      "Epoch 32, loss: 2734.4318132400513\n",
      "Epoch 33, loss: 2730.915614128113\n",
      "Epoch 34, loss: 2727.3973693847656\n",
      "Epoch 35, loss: 2723.671293258667\n",
      "Epoch 36, loss: 2719.468644142151\n",
      "Epoch 37, loss: 2717.504571914673\n",
      "Epoch 38, loss: 2713.8761711120605\n",
      "Epoch 39, loss: 2711.096170425415\n",
      "Epoch 40, loss: 2706.931444168091\n",
      "Epoch 41, loss: 2704.6794443130493\n",
      "Epoch 42, loss: 2702.438359260559\n",
      "Epoch 43, loss: 2698.7244987487793\n",
      "Epoch 44, loss: 2697.5324392318726\n",
      "Epoch 45, loss: 2695.2180557250977\n",
      "Epoch 46, loss: 2690.636239051819\n",
      "Epoch 47, loss: 2690.477437019348\n",
      "Epoch 48, loss: 2687.6634464263916\n",
      "Epoch 49, loss: 2685.435894012451\n",
      "Epoch 50, loss: 2682.2733821868896\n",
      "Epoch 51, loss: 2681.0932216644287\n",
      "Epoch 52, loss: 2679.2474431991577\n",
      "Epoch 53, loss: 2676.489851951599\n",
      "Epoch 54, loss: 2675.4940042495728\n",
      "Epoch 55, loss: 2674.1509103775024\n",
      "Epoch 56, loss: 2671.923345565796\n",
      "Epoch 57, loss: 2670.5356607437134\n",
      "Epoch 58, loss: 2668.2608137130737\n",
      "Epoch 59, loss: 2666.223602294922\n",
      "Epoch 60, loss: 2664.28324508667\n",
      "Epoch 61, loss: 2663.6015129089355\n",
      "Epoch 62, loss: 2661.742232322693\n",
      "Epoch 63, loss: 2660.468403816223\n",
      "Epoch 64, loss: 2659.1579570770264\n",
      "Epoch 65, loss: 2655.5567922592163\n",
      "Epoch 66, loss: 2656.093068599701\n",
      "Epoch 67, loss: 2653.663408279419\n",
      "Epoch 68, loss: 2652.369168281555\n",
      "Epoch 69, loss: 2651.3128700256348\n",
      "Epoch 70, loss: 2647.1848888397217\n",
      "Epoch 71, loss: 2649.020571708679\n",
      "Epoch 72, loss: 2648.1360177993774\n",
      "Epoch 73, loss: 2646.039858818054\n",
      "Epoch 74, loss: 2644.6305589675903\n",
      "Epoch 75, loss: 2643.320749282837\n",
      "Epoch 76, loss: 2642.3576078414917\n",
      "Epoch 77, loss: 2641.8019618988037\n",
      "Epoch 78, loss: 2640.8091259002686\n",
      "Epoch 79, loss: 2639.210654258728\n",
      "Epoch 80, loss: 2638.227551460266\n",
      "Accuracy: 0.800 (4000.0/5000)\n"
     ]
    }
   ],
   "source": [
    "mnist_train = gdata.vision.FashionMNIST(train=True, transform = lambda data, label:\\\n",
    "                                        (data.astype(np.float32)/255.0, label))\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False, transform = lambda data, label:\\\n",
    "                                        (data.astype(np.float32)/255.0, label))\n",
    "net = nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.1))\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.01})\n",
    "batch_size = 64\n",
    "num_epochs = 80\n",
    "ratio = 0.1\n",
    "train_data, test_data = generator(ratio, batch_size)\n",
    "train_data = gluon.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_model(num_epochs, train_data, net, trainer, batch_size, 3./11)\n",
    "test_model(test_data)\n",
    "# this is the weight function we want, save to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.1918093]\n",
       " [ 1.629158 ]\n",
       " [ 1.3973526]\n",
       " ...\n",
       " [10.       ]\n",
       " [ 8.610544 ]\n",
       " [10.       ]]\n",
       "<NDArray 12000x1 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_train, bias_test = preprocess(mnist_train, mnist_test, 6000, .2)\n",
    "X, y = bias_train[:]\n",
    "weight = nd.clip(nd.exp(net(X)), 0.01, 10)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1. + nd.exp(-z))\n",
    "    \n",
    "log_loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "def train_model2(epochs, train_data, net, trainer, batch_size, weight):\n",
    "    for e in range(epochs):\n",
    "        cumulative_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = log_loss(output, label) * weight[i]\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "        print(\"Epoch %s, loss: %s\" % (e, cumulative_loss ))\n",
    "\n",
    "\n",
    "def test_model2(test_data):\n",
    "    num_correct = 0.0\n",
    "    num_total = 0\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        num_total += len(label)\n",
    "        output = net(data)\n",
    "        prediction = ((nd.sign(output).reshape(shape=label.shape) + 1) / 2)\n",
    "        print(prediction, label)\n",
    "        num_correct += nd.sum(prediction == label)\n",
    "    print(\"Accuracy: %0.3f (%s/%s)\" % (num_correct.asscalar()/num_total, num_correct.asscalar(), num_total))\n",
    "    \n",
    "def train_and_test2(num_epochs, train_data, test_data, net, trainer, batch_size,weight):\n",
    "    train_model2(num_epochs, train_data, net, trainer, batch_size, weight)\n",
    "    test_model2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 565.9007399380207\n",
      "Epoch 1, loss: 118.97194454073906\n",
      "Epoch 2, loss: 75.83279372006655\n",
      "Epoch 3, loss: 59.050801530480385\n",
      "Epoch 4, loss: 46.81203496828675\n",
      "Epoch 5, loss: 39.33303553238511\n",
      "Epoch 6, loss: 34.71780950203538\n",
      "Epoch 7, loss: 30.44540436565876\n",
      "Epoch 8, loss: 27.782626791857183\n",
      "Epoch 9, loss: 24.868723805993795\n",
      "Epoch 10, loss: 22.802089791744947\n",
      "Epoch 11, loss: 20.772971882484853\n",
      "Epoch 12, loss: 19.818741157650948\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "total_per_label = 6000\n",
    "ratio = 0.2\n",
    "epochs = 20\n",
    "net = nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.01))\n",
    "train_half, test_half = preprocess(mnist_train, mnist_test, 6000, .2)\n",
    "\n",
    "Train = gluon.data.DataLoader(train_half, batch_size=batch_size, shuffle=True)\n",
    "Test = gluon.data.DataLoader(test_half, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n",
    "\n",
    "train_and_test2(epochs, Train, Test, net, trainer, batch_size, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
